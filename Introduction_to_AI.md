# Readme
- 贵系的人智导，在明理楼 112 教室
- 我个人认为，在贵系，凡是 230 人一同上的课，都是屑课，而且还不签到

  > 我的心里只有感恩

- 马少平老师上课讲了些贵系历史

  > 1. 79 年就有了人工智能与控制这个专业
  > 2. 他做了一辈子的人工智能，从专家系统到深度学习

- 就我所知，大多数贵系老师不知道其他老师讲了什么，而自己还需要讲什么。譬如马老师讲 Dijstra 的时候，还在问我们是否学过，这听上去让我有些遗憾。因为我清楚记得邓公了解其他课( 程设，图形学 )讲了什么，会要求什么

- 我尽量保持不要拖得太久，否则补起来心情会比较忐忑
- 自然，这份笔记是公开的

# Lecture 1 绪论

- 不同于其他研究领域，人工智能有明确的诞生时间

	> 1956 年达特茅斯夏季讨论会
	
- 为什么围棋会比象棋更加难征服？

	> 1. 小的时候不懂事，以为是围棋的计算量高了几个数量级，这有一定道理。	
	>2. 然而实际上的原因是——围棋更加难以判定棋局对谁有利，这对 $\alpha - \beta$ 剪枝算法造成了很大的负面影响，而 AlphaGO 不得不采取 MCTS( 蒙特卡洛树搜索 ) + Deep Learning 来解决。
	
- 人工标记的余年？

	> 1. 还不至于这么早就失业。
	> 2. 因为大多数事情的预判断仍然只属于人类，例如给 training-data 标注男女。
	>2. 棋类运动具有明确的可判定胜负的性质，不需要人工标记。

# Lecture 2 搜索 A

- DFS 的性质

> 1. 一般不能保证找到最优解
> 2. 当深度限制不合理时，可能找不到解，可以将算法改为可变深度限制
> 3. 最坏情况时，搜索空间等同于穷举
> 4. 是一个通用的与问题无关的方法
> 5. 节省内存，只存储从初始节点到当前节点的路径

- BFS 的性质

> 1. 当问题有解时，一定能找到解
> 2. 当问题为单位耗散值，且问题有解时，一定能找到最优解
> 3. 方法与问题无关，具有通用性
> 4. 效率较低
> 5. 存储量比较大

## $A\&A^*$

> 这位更是重量级

- 在 B 站自学的时候，找到了[一个很有意思的博客](https://www.redblobgames.com/pathfinding/a-star/introduction.html#breadth-first-search)，自然加入了我博客的友链里面，还安利给了邓公

<img src="https://s2.loli.net/2022/03/15/alCzyOPiYrHT9Rs.png" alt="image.png" style="zoom:23%;" />

- 我总结下我自己的看法

  > 1. Dijstra 算法与 $A$ 算法具有相同的终止条件——目标到达有限队列的队首。而不太一样的是，**Dijstra 在终点前离开优先队列的节点所算出的距离都是准确的，没有一丝启发**。
  > 1. 贪婪算法则纯粹是启发导向的，完全不考虑实际距离。
  > 1. $A$ 算法是 Dijstra 算法所代表的精确度与贪婪算法所代表的灵活度进行妥协的产物。一般而言，性能与质量都能得到很好的保证。
  > 1. 启发值 $h(x)$ 越大，则启发程度越强，极端情况下退化为贪婪算法，反之，则退化为 Dijstra 算法。
  > 1. 记 $h^*(x)$ 为 $x$ 点到达终点的实际最短距离，我们将满足 $h(x) \le h^*(x)$ 的 $A$ 算法称为 $A^*$ 算法，也即乐观估计原则。
  
- 两个定理

	> 1. 可采纳性定理——若存在从初始节点 s 到目标节点 t 有路径，则 A* 必能找到最佳解结束。
	> 1. 如果 $h_2(n) > h_1(n)$ (目标节点除外)，则 $A_1$ 扩展的节点数 ≥  $A_2$ 扩展的节点数。


- **传教士与恶魔问题**

	> 原标题是传教士与野人问题，还搞了几张黑人和白人一起渡河的图片。
	>
	> 种族歧视，政治不正确⚠️!

有 N 个传教士和 N 个恶魔要渡过一条河，而河中有一条船，船只能容纳 K 个人。而且在任何一个地方（无论是岸边还是船上），如果恶魔的数量多于传教士的数量，恶魔就会吃掉传教士。怎样才能让这些人全都安全过河？（来回的船上都必须要有人操作）

注意，恶魔会完全听从安排，且如果恶魔周围没有传教士，那么不会有人被吃掉。

这个问题又被称为 M-C 问题，M 为传教士人数，C 为恶魔人数。

马老师的课程上， b 表示船的状态，b = 0 即在左岸，b = 1 即在右岸。

# PA0 输入法

- 实现一个简单的汉语拼音输入法，实现从拼音（**全拼**）到汉字（**字符串**）内容的转换。
- 输入
	
	> 1. 多个拼音串储存在指定的文本文件中（input.txt）
	> 1. 每个音之间用空格隔开，不含标点符号、阿拉伯数字和英文等非汉字内容
	> 1. 一行为一句（或一个短语）的拼音串，末尾没有标点符号

- 输出

	> 1. 转换后的汉字串，储存在指定的文本文件中（output.txt）
	> 2. 汉字间没有空格，一行为对应的汉字串，末尾不加标点符号

- ~~就这….~~

- 范围

	> 1. 转换的汉字范围为国标一二级汉字，共 6763 个，以文本文件的形式提供。
2. 训练语料中在该范围之外的汉字可一律不处理，测试语料中保 证均为该范围内的汉字。

- 语料库

	> 1. 【必做】新浪新闻2016年的新闻语料库（见附件 语料库/sina_news_gbk）；
	> 2. 【选做】在GitHub项目nlp_chinese_corpus中选择一个语料库：https://github.com/brightmart/nlp_chinese_corpus 
	> 3. 【选做】自己寻找其他中文语料资源。

- 又不需要卷，而且这个训了没啥成就感，不想玩…

- 很暧昧的评分标准，看了就很不喜欢…

# 神经网络

总觉得上了大二，自己踏实学习的习惯就改变了不少，其一是因为几乎不去教室，其二是因为自己在实验室的学习出现了断裂，听课总是不能听到很高含量的新东西。

不过听 tjy 的话，感觉马老师讲课还算清晰，至少我学会了如何解释 softmax 和 cross entropy 的适应性。

重写下之前写的 [DIDL 笔记](https://zhaochenyang20.github.io/2022/01/15/%E7%A3%95%E7%9B%90/Dive%20Into%20Deep%20Learning%20Part%201/)。

给计 06 的同学讲了讲一些 Python 用法，可以参考这个 [PDF](https://zhaochenyang20.github.io/pdf/python_tutorial.pdf)。

- [最大似然估计](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)
